{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Bank Fraud Detection "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "### The purpose of this proyect is to know if a transaction is fraudulent or not by applying machine learning because banks are often exposed to fraud transactions and constantly improve systems to track them."
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Import the required packages"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "import pandas as pd\r\n",
    "import numpy as np\r\n",
    "import seaborn as sns\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "import matplotlib.cm as cm\r\n",
    "import plotly.express as px\r\n",
    "from collections import Counter\r\n",
    "\r\n",
    "\r\n",
    "import warnings\r\n",
    "warnings.filterwarnings(\"ignore\")"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Import the data and check the data quality"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "data=pd.read_csv('fraud_detection_bank_dataset.csv')\r\n",
    "print(data.head(5))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "   Unnamed: 0  col_0  col_1  col_2  col_3  col_4  col_5  col_6  col_7  col_8  \\\n",
      "0           0      9   1354      0     18      0      1      7      9      0   \n",
      "1           1      0    239      0      1      0      1      0      0      0   \n",
      "2           2      0    260      0      4      0      3      6      0      0   \n",
      "3           3     17    682      0      1      0      0      8     17      0   \n",
      "4           4      1    540      0      2      0      1      7      1      0   \n",
      "\n",
      "   ...  col_103  col_104  col_105  col_106  col_107  col_108  col_109  \\\n",
      "0  ...        0        0        0        1        1        0        0   \n",
      "1  ...        0        1        0        0        0        0        0   \n",
      "2  ...        0        0        0        1        1        0        0   \n",
      "3  ...        0        1        0        1        1        0        0   \n",
      "4  ...        0        0        0        1        1        0        0   \n",
      "\n",
      "   col_110  col_111  targets  \n",
      "0        0       49        1  \n",
      "1        0       55        1  \n",
      "2        0       56        1  \n",
      "3        0       65        1  \n",
      "4        0      175        1  \n",
      "\n",
      "[5 rows x 114 columns]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "source": [
    "print('The number of rows are:',data.shape[0])\r\n",
    "print('The number of columns are:',data.shape[1])\r\n",
    "print('\\n')\r\n",
    "print('Inspect the types of each columns')\r\n",
    "dtypes=data.dtypes.values.tolist()\r\n",
    "dt_count=Counter(dtypes)\r\n",
    "print(dt_count)\r\n",
    "\r\n",
    "\r\n",
    "print('\\n')\r\n",
    "print('Inspect the missing data')\r\n",
    "plt.style.use('fivethirtyeight')\r\n",
    "print('\\n')\r\n",
    "print('Search for missing and null values')\r\n",
    "print(data.isna().sum().sort_values(ascending=False))\r\n",
    "print(data.isnull().sum().sort_values(ascending=False))\r\n"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "The number of rows are: 20468\n",
      "The number of columns are: 114\n",
      "\n",
      "\n",
      "Inspect the types of each columns\n",
      "Counter({dtype('int64'): 113, dtype('float64'): 1})\n",
      "\n",
      "\n",
      "Inspect the missing data\n",
      "\n",
      "\n",
      "Search for missing and null values\n",
      "Unnamed: 0    0\n",
      "col_84        0\n",
      "col_82        0\n",
      "col_81        0\n",
      "col_80        0\n",
      "             ..\n",
      "col_33        0\n",
      "col_32        0\n",
      "col_31        0\n",
      "col_30        0\n",
      "targets       0\n",
      "Length: 114, dtype: int64\n",
      "Unnamed: 0    0\n",
      "col_84        0\n",
      "col_82        0\n",
      "col_81        0\n",
      "col_80        0\n",
      "             ..\n",
      "col_33        0\n",
      "col_32        0\n",
      "col_31        0\n",
      "col_30        0\n",
      "targets       0\n",
      "Length: 114, dtype: int64\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "My first thought was that we have more than 100 features and its a tedious task plot and describe every single feature, so we will probably use a dimensionalty reduction method. But, the data has useful data types and there are no missing values."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "source": [
    "#We delete the first column because its an index\r\n",
    "data.drop(columns='Unnamed: 0',inplace=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "https://towardsdatascience.com/too-much-data-too-little-time-1e7441ecdae1"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "\r\n",
    "primero escalar los datos usando min max escaler y fitting with X (x=data[~target[])\r\n",
    "despues decir que es dificil leer todos los datos en un mismo heatmap pq son muchas columnas \r\n",
    "\r\n",
    "se procede a ocupar PCA y se hace la prueba para saber que cuantos n_components necesitamos y buscamos otras metricas para optimizar el modelo\r\n",
    "procedemos a realizar la comparacion de modelos para cada podelo realizar un gridsearch en busca de sus mejores hyperparaameters\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    " \r\n",
    "finalmente ocupamos un algoritmo de bagging para combinar los modelos y obtener un mejor performance\r\n",
    "\r\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "dataset: https://www.kaggle.com/anasbot/fraud-detection-bank-98-precision\r\n",
    "\r\n",
    "scaler: https://www.geeksforgeeks.org/standardscaler-minmaxscaler-and-robustscaler-techniques-ml/\r\n",
    "\r\n",
    "xgboost: https://data-flair.training/blogs/python-machine-learning-project-detecting-parkinson-disease/\r\n",
    "https://machinelearningmastery.com/develop-first-xgboost-model-python-scikit-learn/\r\n",
    "https://www.kaggle.com/niteshyadav3103/hotel-booking-prediction-99-5-acc\r\n",
    "\r\n",
    "PCA https://www.youtube.com/watch?v=b1NGM3IbRcI\r\n",
    "https://www.kaggle.com/lonewolf95/classification-tutorial-with-pca-and-gridsearchcv\r\n",
    "https://www.kaggle.com/gaborvecsei/pipeline-pca-gridsearch\r\n",
    "\r\n"
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.9.6",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.6 64-bit"
  },
  "interpreter": {
   "hash": "eb10dc7af34d8b2d8e4e13c255a62d72cda2839118676834d127e6a78e1763b3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}